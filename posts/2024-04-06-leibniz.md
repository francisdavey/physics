Prerequisites: calculus

\(
   \def\RR{{\bf R}}
   \def\bold#1{{\bf #1}}
\)

# Introduction
I, along with most students, first learned differential calculus as follows. If $y$ 'is a function of' $x$ then the 'differential' ($y$ differentiated with respect to $x$) is written $$\frac{dy}{dx}$$

This is defined as a limit. $$\frac{dy}{dx}=\lim_{\Delta x \rightarrow 0}\frac{\Delta y}{\Delta x}$$

The $d$'s are suggesting of the $\Delta$'s. But later on, when learning how to "solve" differential equations, we are often presented with this sort of thing:

$$\begin{aligned}
\frac{dy}{dx} &= 2 \\
dy &= 2dx \\\
\int dy &= \int 2xdx \\
y &= x^2 + C \\
\end{aligned}$$

It is natural then to wonder whether you can treat $\frac{dy}{dx}$ as a genuine fraction of $dy$ and $dx$ and, if so, what on Earth are they and are the rules for manipulating them.

## Worked example
Let us take a simple equation. $$y = x^2$$

What does this mean? In our system we are going to say that $y$ and $x$ are functions on the plain. We might say they have type $\mathbb{R}^2 \rightarrow \mathbb{R}$. We will **define** $A=B$ to mean, "the subset of $\mathbb{R}^2$ for which $A - B = 0$. We also need to use *lifting*.

Now let's define $d$. It will take two arguments: a function 

### (alternative story, but really the same thing)
What does $d$ mean? It takes a point and a parametrised curve through any point (explain: parametrised curve) and returns a number, or if you like it takes a congruence of parametrized curves and returns a function at every point. As follows: 

$$df = \frac{f(\gamma(t + \epsilon) - f(t))}{\epsilon}$$

(Or we could define via vectors of course - let's do that).

## Extras 
### Infinitessimals 
An alternative way to deal with $dx$ etc is to treat them as infinitessimals. The very informal idea of an infinitessimal is that it is something so small it can be ignored. There are two main approaches that have been developed rigorously.

* Robinson's non-standard analysis
* Smooth infinitessimal analysis

Robinson's non-standard analysis adds to the reals "infinitessimals". An infinitessimal is smaller than every positive real number but bigger than zero. In other words if $\epsilon$ is an infinitessimal, for every real number $r$: $$0 < \epsilon < r$$ 

You can't have a number that is smaller than every positive number and greater than zero. There are always more numbers between it and zero. You can, for example, halve it. But infititessimals are not numbers so that's OK. You can then work with infinitessimals as much as you like. There is a useful operator "st" which takes the nearest real or "standard part" of a hyperreal so we can think of every hyperreal as standard plus infinitessimals. ...

Smooth infinitessimal analysis....

### Stackexchange references
* [Is dy/dx not a ratio?](https://math.stackexchange.com/questions/21199/is-frac-textrmdy-textrmdx-not-a-ratio/21209#21209) is our main story. The answer by isomorphisms is the right one :-).
* [Failure of differential notation](https://github.com/francisdavey/physics/edit/main/posts/2024-04-06-leibniz.md) use of $d$ in the product rule, one answer discusses Robinson and smooth infinitessimals quite well.
* [What is the practical difference between a differential and a derivative?](https://math.stackexchange.com/questions/23902/what-is-the-practical-difference-between-a-differential-and-a-derivative) the top answer concedes that one can use infinitessimals to make the use of differentials rigorous, but offers a further "suggestive" approach valid "under mild assumptions" (not mentioned) that one can define an operator $d$ but only with an implict functional dependence on $\delta x$.

Basically, we want the "differential" of $y$ to be the infinitesimal change in $y$; this change will be closely approximated to the change along the tangent to $y$; the tangent has slope $y'(a)$. But because we don't have infinitesimals, we have to say *how much* we've changed the argument. So we define "the differential in $y$ at $a$ when $x$ changes by $\Delta x$", $d(y,\Delta x)(a)$, as $d(y,\Delta x)(a) = y'(a)\Delta x$. This is exactly the change along the tangent, rather than along the graph of the function.  If you take the limit of $d(y,\Delta x)$ over $\Delta x$ as $\Delta x\to 0$, you just get $y'$. But we tend to think of the limit of $\Delta x\to 0$ as being $dx$, so abuse of notation leads to "$dy = \frac{dy}{dx}\,dx$"; this is *suggestive*, but not quite true literally; instead, one then can show that arguments that treat differentials as functions tend to give the right answer under mild assumptions. Note that under this definition, you get $d(x,\Delta x) = 1\Delta x$, leading to $dx = dx$.

The problem here is belief that you need implicit functional dependence.

Second answer gives us derivatives = Jacobians and differentials = differential forms. Another answer uses very adavancedlinear algebra to try to addreess what $d$ means. I am not sure if tis works 9?).

<!---
https://arxiv.org/abs/1801.09553 Extending the Algebraic Manipulability of Differentials
https://arxiv.org/abs/2210.07958 Total and Partial Differentials as Algebraically Manipulable Entities
https://arxiv.org/abs/1811.03459 Simplifying and Refactoring Introductory Calculus


-->
