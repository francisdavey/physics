---
layout: post
title: Is dy by dx a fraction?
subtitle: Making sense of Leibniz
date: 4 May 2024
published: true
---

Prerequisites: calculus

$$
   \def\RR{{\bf R}}
$$

$$a=42$$

$$
a = 50
$$


# Introduction
I, along with most students, first learned differential calculus as follows. If $y$ 'is a function of' $x$ then the 'differential' ($y$ differentiated with respect to $x$) is written $\frac{dy}{dx}$.

This is defined as a limit. 

$$\frac{dy}{dx}=\lim_{\Delta x \rightarrow 0}\frac{\Delta y}{\Delta x}$$

The $d$'s are suggesting of the $\Delta$'s. But later on, when learning how to "solve" differential equations, we are often presented with this sort of thing:

$$\begin{aligned}
\frac{dy}{dx} &= 2 \\
dy &= 2dx \\\
\int dy &= \int 2xdx \\
y &= x^2 + C \\
\end{aligned}$$

It is natural then to wonder whether you can treat $\frac{dy}{dx}$ as a genuine fraction of $dy$ and $dx$ and, if so, what on Earth are they and are the rules for manipulating them. As far as I can see many students, and some people advising students, seem to think the answer is "no", but they are quite wrong. 

For example, the following is entirely valid:

$$\begin{aligned}
x^2 + y^2&= C \\
d(x^2 + y^2)&=d(C) \\
2dx + 2dy&=0 \\
\end{aligned}$$

A student can survive fairly well simply knowing a few rules for $d$ and mostly get things right, at least as much as can be expected. There are traps with using this notation, but there are even in elementary algebra. Various people might think this is unsatisfactory:

* It doesn't tell us what $d$ means, so when we encounter a situation not yet covered by our rules we don't know what to do.
* Although some students like to know what the "rules" are, and are happy having no deeper understanding, many really do
* Some people teaching the subject do not seem to know that the notation makes more sense than they think, we might need to justify it
* The whole point of a fun book deep dive is to get to the bottom of things

A college friend of mine, who went on to be a software engineer, thinks of the world from a very strongly computer science perspective. He wants to know what this notation really means in the same way that you would expect to understand the semantics of a python program - or perhaps an ML one. What "type" do all the terms have? What is their meaning

In fact there are at least four ways we can approach this. We will start with a straightforward one that will appeal to a computer scientist. I will then talk about a view that I think underlines the way in which a lot of mathematical physicists and applied mathematicians really use the term, inspired by Arnol'd. Then we might as well look at what Leibniz, who invented the notation, really meant (spoiler: this is where we get infinitessimals) and finally at a rather off the wall approach known as "?".

## Computer science view


## Worked example
Let us take a simple equation. $y = x^2$

What does this mean? In our system we are going to say that $y$ and $x$ are functions on the plain. We might say they have type $\mathbb{R}^2 \rightarrow \mathbb{R}$. We will **define** $A=B$ to mean, "the subset of $\mathbb{R}^2$ for which $A - B = 0$. We also need to use *lifting*.

Now let's define $d$. It will take two arguments: a function 

### (alternative story, but really the same thing)
What does $d$ mean? It takes a point and a parametrised curve through any point (explain: parametrised curve) and returns a number, or if you like it takes a congruence of parametrized curves and returns a function at every point. As follows: 

$$df = \frac{f(\gamma(t + \epsilon) - f(t))}{\epsilon}$$

(Or we could define via vectors of course - let's do that).

## Extras 
### Infinitessimals 
An alternative way to deal with $dx$ etc is to treat them as infinitessimals. The very informal idea of an infinitessimal is that it is something so small it can be ignored. There are two main approaches that have been developed rigorously.

* Robinson's non-standard analysis
* Smooth infinitessimal analysis

Robinson's non-standard analysis adds to the reals "infinitessimals". An infinitessimal is smaller than every positive real number but bigger than zero. In other words if $\epsilon$ is an infinitessimal, for every real number $r$: $0 < \epsilon < r$

You can't have a number that is smaller than every positive number and greater than zero. There are always more numbers between it and zero. You can, for example, halve it. But infititessimals are not numbers so that's OK. You can then work with infinitessimals as much as you like. There is a useful operator "st" which takes the nearest real or "standard part" of a hyperreal so we can think of every hyperreal as standard plus infinitessimals. ...

Smooth infinitessimal analysis....

### Stackexchange references
* [Is dy/dx not a ratio?](https://math.stackexchange.com/questions/21199/is-frac-textrmdy-textrmdx-not-a-ratio/21209#21209) is our main story. The answer by isomorphisms is the right one :-).
* [Failure of differential notation](https://github.com/francisdavey/physics/edit/main/posts/2024-04-06-leibniz.md) use of $d$ in the product rule, one answer discusses Robinson and smooth infinitessimals quite well.
* [What is the practical difference between a differential and a derivative?](https://math.stackexchange.com/questions/23902/what-is-the-practical-difference-between-a-differential-and-a-derivative) the top answer concedes that one can use infinitessimals to make the use of differentials rigorous, but offers a further "suggestive" approach valid "under mild assumptions" (not mentioned) that one can define an operator $d$ but only with an implict functional dependence on $\delta x$.

Basically, we want the "differential" of $y$ to be the infinitesimal change in $y$; this change will be closely approximated to the change along the tangent to $y$; the tangent has slope $y'(a)$. But because we don't have infinitesimals, we have to say *how much* we've changed the argument. So we define "the differential in $y$ at $a$ when $x$ changes by $\Delta x$", $d(y,\Delta x)(a)$, as $d(y,\Delta x)(a) = y'(a)\Delta x$. This is exactly the change along the tangent, rather than along the graph of the function.  If you take the limit of $d(y,\Delta x)$ over $\Delta x$ as $\Delta x\to 0$, you just get $y'$. But we tend to think of the limit of $\Delta x\to 0$ as being $dx$, so abuse of notation leads to "$dy = \frac{dy}{dx}\,dx$"; this is *suggestive*, but not quite true literally; instead, one then can show that arguments that treat differentials as functions tend to give the right answer under mild assumptions. Note that under this definition, you get $d(x,\Delta x) = 1\Delta x$, leading to $dx = dx$.

The problem here is belief that you need implicit functional dependence.

Second answer gives us derivatives = Jacobians and differentials = differential forms. Another answer uses very adavancedlinear algebra to try to addreess what $d$ means. I am not sure if tis works 9?).

<!---
https://arxiv.org/abs/1801.09553 Extending the Algebraic Manipulability of Differentials
https://arxiv.org/abs/2210.07958 Total and Partial Differentials as Algebraically Manipulable Entities
https://arxiv.org/abs/1811.03459 Simplifying and Refactoring Introductory Calculus


-->
